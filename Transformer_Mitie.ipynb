{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1p2BxjTh6XP08wbE9rk7Tl5O6Zp6FdNCl",
      "authorship_tag": "ABX9TyN6DX9u1mXCM5VUzvOZqi7O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kailliang/Text-Classification-with-Transformers/blob/main/Transformer_Mitie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "uAaNOfsYX6cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ViPzvlnmha_",
        "outputId": "9525dcb4-7017-4322-ae94-0698ae8bc6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 30 17:01:54 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import warnings\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, BertTokenizer, BertForSequenceClassification\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "from transformers import MobileBertForSequenceClassification\n",
        "from transformers import AlbertForSequenceClassification\n",
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "# To ignore warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "random_state = 2023\n",
        "epochs = 50\n",
        "batch_size = 256\n",
        "lr=5e-5\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/job_data.csv')\n",
        "\n",
        "# Convert date columns to datetime type\n",
        "date_columns = ['reported_date', 'target_finish', 'actual_finish']\n",
        "\n",
        "# Drop rows with invalid datetime values\n",
        "\n",
        "for col in date_columns:\n",
        "    data[col] = pd.to_datetime(data[col], dayfirst=True, errors='coerce')\n",
        "\n",
        "# Drop rows with invalid datetime values\n",
        "data = data.dropna(subset=date_columns)\n",
        "\n",
        "\n",
        "# Preprocessing: Handling missing data and encoding categorical variables\n",
        "data['raised_within_workhours'].fillna('Unknown', inplace=True)\n",
        "data['location_type'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# Encoding categorical variables\n",
        "le = LabelEncoder()\n",
        "data['raised_within_workhours'] = le.fit_transform(data['raised_within_workhours'])\n",
        "# data['location_type'] = le.fit_transform(data['location_type'])\n",
        "\n",
        "data.loc[:, 'location_type'] = le.fit_transform(data.loc[:, 'location_type'])\n",
        "\n",
        "data['expected_duration'] = ((pd.to_datetime(data['target_finish']) - pd.to_datetime(data['reported_date'])).dt.total_seconds() / (3600 * 24)).astype(np.int64)\n",
        "\n",
        "# Lable\n",
        "data['on_time'] = (data['target_finish'] - data['actual_finish']).dt.total_seconds() / 3600 / 24\n",
        "# data['on_time'] = data['on_time'].apply(lambda x: \"Was the job expected to be finished on time: 1\" if x > 0 else \"Was the job expected to be finished on time: 0\") \n",
        "data['on_time'] = data['on_time'].apply(lambda x: 1 if x > 0 else 0) \n",
        "\n",
        "# Create time features\n",
        "data['reported_hour'] = data['reported_date'].dt.hour\n",
        "data['reported_day_of_week'] = data['reported_date'].dt.dayofweek\n",
        "data['reported_month'] = data['reported_date'].dt.month\n",
        "data['reported_year'] = data['reported_date'].dt.year\n",
        "\n",
        "data['target_hour'] = data['target_finish'].dt.hour\n",
        "data['target_day_of_week'] = data['target_finish'].dt.dayofweek\n",
        "data['target_month'] = data['target_finish'].dt.month\n",
        "data['target_year'] = data['target_finish'].dt.year\n",
        "\n",
        "\n",
        "# Convert to sentences\n",
        "def convert_to_sentence(row):\n",
        "    # return f\"Job was reported with priority: {row['priority']}, location type: {row['location_type']}, was it raised within workhours: {row['raised_within_workhours']}. The reported hour is {row['reported_hour']}, day of week is {row['reported_day_of_week']}, month is {row['reported_month']}, and year is {row['reported_year']}. The target hour is {row['target_hour']}, day of week is {row['target_day_of_week']}, month is {row['target_month']}, and year is {row['target_year']}.\"\n",
        "    # return f\"Job was reported with priority: {row['priority']}, The reported hour is {row['reported_hour']}, day of week is {row['reported_day_of_week']}, month is {row['reported_month']}, and year is {row['reported_year']}. The target hour is {row['target_hour']}, day of week is {row['target_day_of_week']}, month is {row['target_month']}, and year is {row['target_year']}.\"\n",
        "    return f\"Job reported at date: {row['reported_date']}, with priority: {row['priority']}, location type: {row['location_type']}, was it raised within workhours: {row['raised_within_workhours']}, and target finish time: {row['target_finish']}.\"\n",
        "    # return f\"Job reported at date: {row['reported_date']}, with priority: {row['priority']}, and target finish time: {row['target_finish']}.\"\n",
        "\n",
        "\n",
        "data['text'] = data.apply(convert_to_sentence, axis=1)\n",
        "\n",
        "\n",
        "X = data['text']\n",
        "y = data['on_time']\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
        "input_ids = [torch.tensor(tokenizer.encode(sent, add_special_tokens=True)) for sent in X]\n",
        "\n",
        "input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i != 0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, y, random_state=42, test_size=0.2)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.2)\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "train_labels_encoded = encoder.fit_transform(train_labels)\n",
        "validation_labels_encoded = encoder.transform(validation_labels)\n",
        "\n",
        "train_labels = torch.tensor(train_labels_encoded)\n",
        "validation_labels = torch.tensor(validation_labels_encoded)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Compute class weights\n",
        "counts = np.bincount(train_labels_encoded)\n",
        "class_weights = 1. / counts\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "\"\"\"\n",
        "models -------------------------------------------------------------------------------------------------------------------\n",
        "\"\"\"\n",
        "\n",
        "# model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(np.unique(train_labels_encoded)))\n",
        "\n",
        "# model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=len(np.unique(train_labels_encoded)))\n",
        "\n",
        "# model = MobileBertForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=len(np.unique(train_labels_encoded))) # 147M\n",
        "\n",
        "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(np.unique(train_labels_encoded)))\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(np.unique(train_labels_encoded)))\n",
        "\n",
        "\"\"\"\n",
        "------------------------------------------------------------------------------------------------------------------------ \n",
        "\"\"\"\n",
        "loss_function = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader)*epochs)\n",
        "\n",
        "# Train the model\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):  \n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for i, batch in enumerate(train_dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_mask = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    total_loss += loss.item()  # accumulate the loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "  avg_train_loss = total_loss / len(train_dataloader)  # calculate the average loss over all batches\n",
        "\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  for batch in validation_dataloader:\n",
        "    input_ids = batch[0].to(device)\n",
        "    attention_mask = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    predicted = torch.argmax(logits, dim=1)\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "    predictions.extend(predicted.detach().cpu().numpy())\n",
        "    true_labels.extend(labels.detach().cpu().numpy())\n",
        "  if epoch%10 == 0:\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item()}')\n",
        "    print('Accuracy of the model on the test set: %d %%' % (100 * correct / total))\n",
        "  # Compute confusion matrix\n",
        "    cf_matrix = confusion_matrix(true_labels, predictions)\n",
        "    print('Confusion Matrix: \\n', cf_matrix)\n",
        "\n",
        "\n",
        "\n",
        "# Compute precision\n",
        "precision = precision_score(true_labels, predictions, average='weighted')\n",
        "print('Precision: ', precision)\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "lb.fit(true_labels)\n",
        "true_labels_bin = lb.transform(true_labels)\n",
        "predictions_bin = lb.transform(predictions)\n",
        "fpr, tpr, _ = roc_curve(true_labels_bin.ravel(), predictions_bin.ravel())\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC: ', roc_auc)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8zOXSS5ta4kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the confusion matrix\n",
        "cm = np.array([[30, 54], [44, 164]])\n",
        "\n",
        "# Define the labels\n",
        "labels = ['Positive', 'Negative']\n",
        "\n",
        "# Create a dataframe for a better visualization\n",
        "df_cm = pd.DataFrame(cm, columns=labels, index=labels)\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(df_cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FGDsOVNhvkW2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}